{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo methods\n",
    "\n",
    "Don't forget to save your progress during the datalab to avoid any loss due to crashes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this datalab we are going to get acquainted with the basics of Monte Carlo particle transport methods, and we will learn how to sample random events and random values from various distributions.\n",
    "\n",
    "The new python knowledge from the lab is going to be \n",
    "- random number generators from `numpy.random`\n",
    "\n",
    "Let's get started and have some fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "Any given result from a Monte Carlo calculation, colloquially known as a “tally”, represents an estimate of the mean of some random variable of interest. This random variable typically corresponds to some physical quantity like a reaction rate, a net current across some surface, or the neutron flux in a region. Given that all tallies are produced by a stochastic process, there is an associated uncertainty with each value reported. It is important to understand how the uncertainty is calculated and what it tells us about our results. To that end, we will introduce a number of theorems and results from statistics that should shed some light on the interpretation of uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law of Large Numbers\n",
    "\n",
    "The law of large numbers is a fundamental statistical theorem that states that the average value of the results of a large number of repeated experiments will converge to the expected value as the number of trials increases.\n",
    "\n",
    "Let $ X_1, X_2, \\dots, X_n$ be an infinite sequence of independent, identically-distributed random variables with expected values $ E(X_1) = E(X_2) = \\mu $. \n",
    "\n",
    "One form of the law of large numbers states that the sample mean \n",
    "\n",
    "$$\n",
    "\\overline{X}_n = \\frac{X_1 + \\cdots + X_n}{n}\n",
    "$$\n",
    "\n",
    "converges in probability to the true mean $ \\mu $. That is, for all $ \\epsilon > 0 $:\n",
    "\n",
    "$$\n",
    "P(|\\overline{X}_n - \\mu| > \\epsilon) \\to 0 \\quad \\text{as } n \\to \\infty.\n",
    "$$\n",
    "\n",
    "This result highlights that as the number of trials $ n $ increases, the sample mean becomes a more accurate estimate of the population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size\n",
    "N = 1000000\n",
    "\n",
    "# Simulate rolling a die\n",
    "true_mean_die = 3.5  # The true mean of a fair six-sided die\n",
    "rolls = np.random.randint(1, 7, size=N)  # Generate N rolls of a six-sided die\n",
    "\n",
    "# Compute the running mean\n",
    "running_mean_die = np.cumsum(rolls) / np.arange(1, N + 1)\n",
    "\n",
    "difference_die = # TODO Compute the absolute difference between the estimated mean and the true mean\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the running mean\n",
    "# TODO Plot the running mean and the true mean\n",
    "ax[0].set_xlabel('Sample Size')\n",
    "ax[0].set_ylabel('Mean')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_title('Convergence of Sample Mean to True Mean (Die Rolls)')\n",
    "ax[0].legend()\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot the difference\n",
    "# TODO Plot the absolute difference between running mean and true mean\n",
    "# TODO plot the 1/sqrt(N) line for comparison\n",
    "ax[1].set_xlabel('Sample Size')\n",
    "ax[1].set_ylabel('Absolute Difference')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_title('Difference Between Estimated and True Mean (Die Rolls)')\n",
    "ax[1].legend()\n",
    "ax[1].grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Limit Theorem (CLT)\n",
    "\n",
    "The Central Limit Theorem (CLT) is one of the most important results in probability and statistics. It describes the behavior of the sample mean of a large number of independent, identically-distributed random variables.\n",
    "\n",
    "The CLT states that, given a sequence of independent and identically-distributed random variables $ X_1, X_2, \\dots, X_n $ with:\n",
    "\n",
    "- Expected value $ E(X_i) = \\mu $\n",
    "- Variance $ \\text{Var}(X_i) = \\sigma^2 < \\infty $\n",
    "\n",
    "the distribution of the sample mean $ \\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i $ will approach a normal distribution as the sample size $ n \\to \\infty $, regardless of the original distribution of the $ X_i $.\n",
    "\n",
    "More formally, the random variable:\n",
    "\n",
    "$$\n",
    "Z = \\sqrt{n} \\left( \\overline{X}_n - \\mu \\right) / \\sigma\n",
    "$$\n",
    "\n",
    "converges in distribution to the standard normal distribution $ N(0, 1) $ as $ n \\to \\infty $.\n",
    "#### Key Implications:\n",
    "1. The CLT allows us to make inferences about population parameters using sample statistics, even if the population distribution is not normal.\n",
    "2. It is the foundation for many statistical methods, including hypothesis testing and confidence intervals.\n",
    "\n",
    "#### Example:\n",
    "If we repeatedly take samples of size $ n $ from a population with mean $ \\mu $ and variance $ \\sigma^2 $, the distribution of the sample means will approximate a normal distribution as $ n $ becomes large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of dice to roll\n",
    "num_dice = # TODO Define the number of dice to roll in a list\n",
    "\n",
    "# Number of trials for each case\n",
    "num_throws = 100000\n",
    "\n",
    "# Plot the distributions\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, n in enumerate(num_dice):\n",
    "    # Simulate rolling 'n' dice 'num_throws' times\n",
    "    sample_means = np.mean(np.random.randint(1, 7, size=(num_throws, n)), axis=1)\n",
    "    \n",
    "    # Plot the histogram of the sums\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    # TODO plot a histogram of the sample mean\n",
    "    plt.title(f'{n} Dice')\n",
    "    plt.xlabel('Sample Mean')\n",
    "    plt.ylabel('Probability Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the die is biased, the distribution of the sample mean will also converge to a normal distribution, but the mean and variance will be different.\n",
    "\n",
    "# Number of samples to draw\n",
    "num_samples = # TODO Define the number of dice to roll in a list\n",
    "\n",
    "# Number of trials for each case\n",
    "numb_throws = 1000\n",
    "\n",
    "# Plot the distributions\n",
    "fig, axs = plt.subplots(1, len(num_samples), figsize=(15, 4))\n",
    "\n",
    "for i, n in enumerate(num_samples):\n",
    "    # Simulate drawing 'n' samples from a non-normal distribution 'num_trials' times\n",
    "    sample_means = [np.mean(np.random.choice([1, 2, 3, 4, 5, 6], p=[0.6, 0.2, 0.1, 0.05, 0.025, 0.025], size=n)) for _ in range(numb_throws)]\n",
    "    \n",
    "    # Plot the histogram of the sample means\n",
    "    # TODO plot a histogram of the sample mean\n",
    "    axs[i].set_title(f'{n} Samples')\n",
    "    axs[i].set_xlabel('Sample Mean')\n",
    "    axs[i].set_ylabel('Probability Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from distributions\n",
    "\n",
    "Let's apply this to a more neutronically relevant observable...\n",
    "\n",
    "### Discrete distribution: which event happens?\n",
    "\n",
    "The probability of reaction $i$ happening at energy $E$ is \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\Sigma_i(E)}{\\Sigma_t(E)}\n",
    "\\end{equation}\n",
    "\n",
    "Let us consider that in our material only two reactions might happen: scattering or capture, thus a simple condition can be used to decide which happens.\n",
    "\n",
    "Complete the `reactionType` function to return a random event type. Assume that at the energy the neutron is travelling with $\\Sigma_s=0.64 \\: \\text{cm}^{-1}$ and $\\Sigma_c=0.39 \\: \\text{cm}^{-1}$. Call the function with these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactionType(SigS,SigC):\n",
    "    \"\"\"Function to sample a random event type\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    SigS : float\n",
    "        Macroscopic scattering cross section\n",
    "    SigC : float\n",
    "        Macroscopic capture cross section\n",
    "    \"\"\"\n",
    "    SigT= # TODO Complete the line\n",
    "    x= #TODO Sample random number between 0 and 1\n",
    "    if x < SigS/SigT:\n",
    "        return 'scatter'\n",
    "    # TODO else return 'capture'\n",
    "    #\n",
    "\n",
    "ss=0.64\n",
    "sc=0.39\n",
    "print()# TODO Complete the line with the function call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy actually has a built in function `np.random.choice()`, which does the same for us. As an input it takes a list of choices to sample from, and optionally one can also pass a list of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(['scatter','capture'],p=[ss/(ss+sc),sc/(ss+sc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continous distribution I: path to next collision\n",
    "\n",
    "Let's consider that we have some probability density function $p(x)$, and the related cumulative distribution function is $F(x)=\\int_{-\\infty}^xp(t)dt$. This function is going to take values between 0 and 1. So if we can sample random numbers uniformly between 0 and 1, we could just convert them by evaluating the inverse of the cumulative distribution function to obtain a random value $x$ sampled from the distribution:\n",
    "\n",
    "$x=F^{-1}(r)$\n",
    "\n",
    "This of course is only useful, when it is possible to easily integrate the probability density function.  \n",
    "\n",
    "Let's see how we can use this to sample random distances travelled by a neutron between collision events. We learnt that \n",
    "\n",
    "$\\exp(-\\Sigma_t x)$ is the probability that a neutron moves a distance dx without any interaction.\n",
    "\n",
    "and \n",
    "\n",
    "$\\Sigma_t \\exp(-\\Sigma_t x)dx$ is the probability that the neutron has its interaction at dx.\n",
    "\n",
    "So\n",
    "\n",
    "$p(x)=\\Sigma_t \\exp(-\\Sigma_t x)$\n",
    "\n",
    "Thus\n",
    "\n",
    "$F(x)=1-\\exp(\\Sigma_tx)$\n",
    "\n",
    "If we take the inverse, to sample a random path\n",
    "\n",
    "$x=-\\frac{\\ln(1-r)}{\\Sigma_t}$\n",
    "\n",
    "but if r is uniform over $[0,1]$, than $1-r$ is also uniform over $[0,1]$, so this simplifies to\n",
    "\n",
    "$x=-\\frac{\\ln r}{\\Sigma_t}$\n",
    "\n",
    "Complete the `distanceToCollision` function below.\n",
    "\n",
    "**Note #1** computational speed is everything in MC calculations. Although in this course we don't try to avoid every unnecessary operation, this example is just to highlight that sometimes operations can be avoided with some reasoning.\n",
    "\n",
    "**Note #2** the natural logarithm can be computed with `np.log`.\n",
    "\n",
    "**Note #3** `numpy.random` has a built-in function to sample the exponential distribution, nevertheless here we will convert the uniformly distributed random numbers between $[0,1]$ to exponentially distributed random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceToCollision(SigT,N=1):\n",
    "    \"\"\"Function to sample the distance between collisions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    SigT : float\n",
    "        Total Macroscopic cross section in 1/cm\n",
    "    N : int\n",
    "        Number of events to be sampled (default=1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : float or array-like\n",
    "        Random distance between collisions\n",
    "    \"\"\"\n",
    "    r = np.random.uniform(0,1,N)\n",
    "    x = # TODO Complete the line\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try this function. Let's consider that 1 MeV neutrons enter a material which has a total cross section of $\\Sigma_t=0.18 \\: \\text{cm}^{-1}$ at this energy. Or well, let's consider that 10k neutrons enter the material, and let's see how the distribution of the random distances looks like, and what is the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SigT=0.18\n",
    "N=10000\n",
    "ds= # TODO Call distanceToCollision() here\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(ds,100)\n",
    "plt.show()\n",
    "\n",
    "print('Empirical Mean (cm) \\t Theoretical mean (cm)')\n",
    "print() # TODO Print the empirical mean free path. and the mean free path expected from theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continous distribution II: Watt distribution\n",
    "\n",
    "\n",
    "When the probability density function is less well behaving, and we cannot obtain the cumulative distribution function easily, we can use for example the rejection method. In this case, we draw a random number (r1), convert it to be between $a$ and $b$ (the bounds of the random value), then we draw an other random number (r2) to create a $y$ value based on the maximum of the probaility density function (M). If the $(x,y)$ pair is under the curve (ie. $y<p(x)$) we accept the value. \n",
    "\n",
    "<img src=\"rejection.png\" width=\"200\"/>\n",
    "\n",
    "**Note** This might be very inefficient if the probability density function is peaked. There are several other methods to more efficient sampling.\n",
    "\n",
    "Let's try to use this method for sampling the Watt-spectrum which is the probability density function of the energy of neutrons emerging from fission.\n",
    "\n",
    "$$\\chi (E)=C_1\\cdot \\exp(-\\frac{E}{C_2})\\cdot \\sinh(\\sqrt{C_3\\cdot E})$$\n",
    "\n",
    "Draw 100 numbers $x$ between 0-10 MeV and draw 100 numbers $y$ between 0 and the maximum of $\\chi(E)$. If the sampled energy is accepted, plot the $(x,y)$ coordinate with green, else with red.\n",
    "\n",
    "Does this method seem to be efficient to sample the Watt-spectrum? Count the number of accepted random samples to estimate the efficiency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watt(x): \n",
    "    C1 = 0.453\n",
    "    C2 = 0.965\n",
    "    C3 = 2.29\n",
    "    return # TODO Complete the line\n",
    "                                \n",
    "E=np.linspace(0,10,10000)\n",
    "plt.figure()\n",
    "plt.plot(E,watt(E))\n",
    "maxW=np.max(watt(E))\n",
    "\n",
    "for _ in range(100):\n",
    "    xi=np.random.uniform(0,10)\n",
    "    yi= # TODO Complete the line\n",
    "    if yi<watt(xi):\n",
    "        plt.plot(xi,yi,'gx')\n",
    "    # TODO Complete the if/else statements\n",
    "    # TODO Count how often a number is accepted!\n",
    "    \n",
    "plt.xlabel('Energy (MeV)')\n",
    "plt.ylabel(r'$\\chi (MeV^{-1})$')\n",
    "plt.show()\n",
    "\n",
    "print() # TODO Print the estimated efficiency "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (sandy-devel)",
   "language": "python",
   "name": "sandy-devel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
